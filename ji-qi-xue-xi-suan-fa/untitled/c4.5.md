# C4.5

对于ID3存在的几个问题，C4.5是这样处理的

* 如何处理连续特征？将连续特征离散化，假如说某连续特征是$$\{1,2,3,4,5,6,7,8,9\}$$共9个数，那么我们取相邻两个数的平均值$$\{1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5\}$$，就会产生8个分割点，分别计算以每个分割点为中心进行分割时的信息增益，例如以3.5为划分点的话，小于3.5为第一类，大于3.5为第二类。选择信息增益最大的分割点。关于连续值处理详细请见：[连续值处理]()
* 将信息增益率作为特征选择的标准，这样按照公式样本越多分母就会越大，计算的数值就越小
* 对缺失值的处理主要分为两个部分：（关于缺失值处理详细请见：[缺失值处理]()）
  * 一是对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。
  * 二是选定了划分属性，对于在该属性上缺失特征的样本的处理。可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9

虽然C4.5解决了ID3中的大部分问题，但还有很多改进的空间。

* 决策树算法非常容易过拟合，所以要对决策树进行剪枝。主要有两种剪枝的方式：1. 预剪枝 2. 后剪枝
* C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
* C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
* C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。

