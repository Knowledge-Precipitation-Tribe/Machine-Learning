# 预剪枝

基于信息增益准则，我们首先选选取“脐部“对训练集进行划分，产生来三个分支，如图所示。

![](../../../.gitbook/assets/image%20%2819%29.png)

然后是否在这个基础上继续进行划分呢，就要用到模型评估。

假设对于第一个节点，我们不进行划分，直接将它算作叶节点的话，那么他的类别就是训练集合中数量最多的种类，就是好瓜，那么我们在验证数据上验证一下其精度，其中编号$$\{4,5,8\}$$，分类正确，其余四个分类错误，所以其精度为$$\frac{3}{7} \times 100 \%=42.9 \%$$.

在用属性脐部划分之后，上图中的节点2，3，4分别包含编号为{1,2,3,14},{6,7,15,17},{10,16}的训练样例。因此这三个节点被标记为叶节点“好瓜“，“好瓜“，“坏瓜“。此时，在验证数据集中编号为{4，5，8，11，12}的样例被划分正确，此时验证集的精度为$$\frac{5}{7} \times 100 \%=71.4 \%>42.9 \%$$，于是，使用“脐部“划分是正确的。

然后，决策树算法应该对结点②进行划分,基于信息增益准则将挑选出划 分属性“色泽”。然而，在使用“色泽”划分后，编号为{5}的验证集样本分类 结果会由正确转为错误,使得验证集精度下降为57.1%.于是,预剪枝策略将禁 止结点②被划分. 

对结点③,最优划分属性为“根蒂”，划分后验证集精度仍为71.4%.这个 划分不能提升验证集精度,于是,预剪枝策略禁止结点③被划分. 

对结点④,其所含训练样例已属于同一类,不再进行划分. 

于是,基于预剪枝策略从表4.2数据所生成的决策树如图4.6所示,其验证集精度为71.4%.这是一棵仅有一层划分的决策树,亦称“决策树桩”\(decision stump\). 

对比图4.6和图4.5可看出，预剪枝使得决策树的很多分支都没有“展开”,这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测 试时间开销.但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降,但在其基础上进行的后续划分却有可能导致性能显著提高；预剪枝基于贪心本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。

