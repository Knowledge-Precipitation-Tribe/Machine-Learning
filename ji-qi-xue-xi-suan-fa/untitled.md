# 决策树

在介绍决策树之前我们先看看决策树长什么样。我们这个决策树是判定一个人是否要去相亲。​

![](https://gblobscdn.gitbook.com/assets%2F-M57X797HGatojkUYCZC%2F-M57XHbKgpkKcLHg1jSR%2F-M57YaM8e0L-UqRlMBAe%2Fimage.png?alt=media&token=aee856e3-b70c-4a76-b5eb-7057c4a7b9f5)

学过软件工程的朋友应该接触过判定树，判定树也称为决策树，只不过呢在机器学习的决策树中如何向下分割节点要由更复杂的计算公式决定。

**我们先来看一下决策树的整体流程：**

* 我们观察决策树可以发现，决策树是一个从根节点到叶子结点逐渐递归的过程
* 在每个中间节点我们要找到一个**划分的属性**。这个划分的属性就是我们一会要重点解释的，也就是决策树是怎么划分出来的。

再解释如何寻找可以划分的属性之前，我们**再来分析一下决策树分到什么程度可以停止：**

* 当前节点包含的样本全属于同一类别，无需再进行划分
* 当前节点划分之后这个属性为空，或者是样本的所有属性取值都相同，无法划分
* 当前节点包含的样本集合为空，不能划分

_注：这里说的属性也就是每个样本的特征。_

确定了整体流程，也确定了最后的终止条件，我们就来看一下决策树中的属性划分是如何决定的。

按照属性划分的原理不同我们可以划分出**三种不同的决策树**：

* 按照信息增益分割节点：就是ID3决策树
* 按照信息增益率分割节点：就是C4.5决策树
* 按照基尼系数分割节点：就是CART决策树

在介绍这几种决策树之前我们先引入一个**信息熵**的概念

