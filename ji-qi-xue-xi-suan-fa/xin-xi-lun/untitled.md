# Entropy

## 信息熵 <a id="xin-xi-shang"></a>

所谓的信息熵就是度量一个样本集合"纯度/不确定度"的指标，如何理解呢,我们来举个例子：

假设你在医生办公室的候诊室里和三个病人谈话。 他们三个人都刚刚完成了一项医学测试，经过一些处理，产生了两种可能的结果之一: 疾病要么存在，要么不存在。 他们已经提前研究了特定风险概率，现在急于找出结果，病人 a 知道，根据统计，他有95% 的可能性患有这种疾病。 对于病人 b，被诊断为患病的概率是30% 。 相比之下，患者 c 的概率是50 %。

现在我们想集中讨论一个简单的问题。 在其他条件相同的情况下，这三个病人中哪一个面临最大程度的不确定性？

答案很清楚: 病人 c 经历了"最多的不确定性"。 在这种情况下，他所经历的是最大程度的不确定性: 就像抛硬币一样。但是我们如何精确的来计算这种不确定度呢？就有了下面这个公式：

$$H(D)=H(p1,…,pn)=−∑i=1npilog⁡2piH(D)=H\left(p_{1}, \ldots, p_{n}\right)=-\sum_{i=1}^{n} p_{i} \log _{2} p_{i}H(D)=H(p1​,…,pn​)=−i=1∑n​pi​log2​pi​$$

其中pip{i}pi_就是第_iii _个事件发生的概率，也可以看作在整个集合中第_iii_类样本所占的比例。规定若_pi=0p{i}=0pi=0_​_pilog⁡2pi=0p{i}\log{2}p{i}=0pilog2pi=0_。计算出的信息熵最小值为0，最大值为_log⁡2n\log {2}\mathcal{n}log2n，当我们计算出的**结果越小**，代表当前这个**数据越纯**，也就是**不确定度越低**。

我们在用一个具体的例子来解释信息熵是如何计算的：

![](https://gblobscdn.gitbook.com/assets%2F-M57X797HGatojkUYCZC%2F-M57XHbKgpkKcLHg1jSR%2F-M57ZL0zWRGPJVp71DoI%2Fimage.png?alt=media&token=869c8ca3-1937-4e48-87a5-4a0ca564141a)

在这个例子中我们数据集共有17条数据，其中包含好瓜和坏瓜两个类别，其中好瓜/正例占p1=817p{1}=\frac{8}{17}p1=178​ _，坏瓜/负例占_p2=917p{2}=\frac{9}{17}p2=179​。

那么现在这个集合的信息熵为：

H\(D\)=−∑i=12pilog⁡2pi=−\(817log⁡2817+917log⁡2917\)=0.998H\(D\)=-\sum\_{i=1}^{2} p\_{i} \log \_{2} p\_{i}= -\left\(\frac{8}{17} \log \_{2}^{\frac{8}{17}}+\frac{9}{17} \log \_{2}^{\frac{9}{17}}\right\)=0.998H\(D\)=−i=1∑2​pi​log2​pi​=−\(178​log2178​​+179​log2179​​\)=0.998

可以看到我们当前的这个数据集合计算出来的值很大，也就代表当前的数据还是很混乱的，因为正负两个样本基本上各占一半。

## 信息熵的最大与最小值 <a id="xin-xi-shang-de-zui-da-yu-zui-xiao-zhi"></a>

信息熵的取值范围

​

### 最大值： <a id="zui-da-zhi"></a>

​

### 最小值 <a id="zui-xiao-zhi"></a>

​

